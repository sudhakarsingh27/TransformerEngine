{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e567b9a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 30) (2359419807.py, line 30)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 30\u001b[0;36m\u001b[0m\n\u001b[0;31m    - screenshots from nsys aren't great. The profiles could be hard to read. Annotate?\u001b[0m\n\u001b[0m                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 30)\n"
     ]
    }
   ],
   "source": [
    "[x] no hypen in T-E\n",
    "2. Goal should be in Goal\n",
    "3. Use .. instead of \"things to note...\" for sphinx to create a \"note\"\n",
    "    - Compress to a sentence or two\n",
    "    - grammatical error in the note section\n",
    "4. ToC \n",
    "    - kinda okay\n",
    "    - 6. Tutorial part -> make it more telling\n",
    "5. make change to rst\n",
    "6. \"Transformer\" arch\n",
    "7. link meta ai llama link in \"llama-2\"\n",
    "8. \"A lot is already...\" is not a good statement, be more useful to the user\n",
    "9. \"we consider llama2\" should not be used multiple places\n",
    "10. Link llama2 to its proper place\n",
    "11. More about the model is out of context\n",
    "    - have a diagram about going from decoder to llama arch\n",
    "    - link to a good llama tutorial\n",
    "    - \n",
    "12. Huggingface is not the \"go to\" place (probably NeMo is)\n",
    "13. Have all those imports/functions in `te_llama.py` file maybe\n",
    "    - encapsulate the imports in `te_llama.py` or maybe have another utils.py\n",
    "    - Average time over steps \n",
    "    - Could have a building table \n",
    "14. How to run with `FP8` as compared to `BF16` case \n",
    "    - Show how TELinear are run in `FP8` precision\n",
    "    - From utils, do the funcs. The main difference is in showing how FP8 version is run\n",
    "    - compress the code that actually trains/finetunes\n",
    "15. Add the explanation for the the improvement 1\n",
    "    - Be more formal\n",
    "    - screenshots from nsys aren't great. The profiles could be hard to read. Annotate? \n",
    "    - Show the llama decoder vs te llama decoder in the profile, show cpu vs gpu activity (remove extra gpu info)\n",
    "    - Could show that fp8 gemm is shorter than bf16 shorter in the annotation\n",
    "        - gemms fp8 are faster but there are casts involved\n",
    "        - Because of the CPU work\n",
    "16. Nice segue into using larger layers in TE\n",
    "17. (N in layernorm should be capital), name of `Attention` and `MLP` are wrong as per TE\n",
    "18. Maximum thing that is provided is the full TransformerLayer\n",
    "    - substituting the full TransformerLayer gives us good results\n",
    "19. Introduce the transformerLayer after the Full transformerlayer intro\n",
    "20. REmove the `MyGPT` thingy\n",
    "21. Linking TE docs as maybe note\n",
    "    - Here we're explorign the options needed for Llama, but TL has more options that are explained further in the API docs\n",
    "22. \"bunch of options\" is not formal\n",
    "23. Add the `te_llama.py` explained to `TransforerLayer` options explained\n",
    "    - no way to use the `transformerlayer` directly in HF models but the options are kinda explainable.\n",
    "    - the relevant options are explained as follows\n",
    "    - The names of the weights are different, so need to take care of remapping\n",
    "24. mapping of the weights is okay, add the links to the modules from hf llama modeling py\n",
    "    - instead saying that we're just replacing the layers here.\n",
    "25. monkey patching function showed in the tutorial as well (as excerpts)\n",
    "26. \"this .. runs .. Bf16\" should be in a text somewhere. \n",
    "    - with that we have our next speedup\n",
    "    - we see that even with BF16, it's still faster\n",
    "27. Final version of the table is okay\n",
    "28. change the ToC accordingly\n",
    "29. more mindful of rst\n",
    "    - newlines before and after the lists\n",
    "    - rst is more syntax demanding\n",
    "30. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5461b1ad",
   "metadata": {},
   "source": [
    "# Accelerate HF Llama model with TransformerEngine\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "<b>Goal</b>\n",
    "\n",
    "This tutorial showcases three incrementally efficient ways to use [TransformerEngine library](https://github.com/NVIDIA/TransformerEngine) to finetune (full) a Llama2 model from [Hugging Face](https://huggingface.co/meta-llama/Llama-2-7b-hf).\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824aea22",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "<b>Note</b>\n",
    "    \n",
    "This tutorial showcases finetuning a full 7B Llama2 model (https://huggingface.co/meta-llama/Llama-2-7b-hf) on h100 GPUs (which have 80GB of HBM). Therefore, running the following individual portions (Baseline, Improvement 1, Improvement 2 and Improvement 3) for perf benchmarking will require restarting the Jupyter notebook kernel each time.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12beb80",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "1. From \"Transformer\" to \"Llama\"\n",
    "2. Hugging Face's `LlamaModel`\n",
    "    - Hugging Face's `LlamaDecoderLayer`\n",
    "3. TransformerEngine's `TransformerLayer`\n",
    "    - `TransformerLayer` options explained\n",
    "4. Necessary Imports\n",
    "5. [Baseline] Running HF `LlamaModel` (Precision: `BF16`)\n",
    "6. [Improvement 1] Replace `nn.Linear` with TE's `Linear` layers (Precision: FP8)\n",
    "7. [Improvement 2] Replace HF's `LlamaDecoderLayer` with TE's `TransformerLayer` (Precision: `BF16`)\n",
    "8. [Improvement 3] Replace HF's `LlamaDecoderLayer` with TE's `TransformerLayer` (Precision: `FP8`)\n",
    "9. Benchmarks revisited and Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e89d6f",
   "metadata": {},
   "source": [
    "### Understanding the aberration with Improvement 2 (CPU Overheads)\n",
    "\n",
    "<figure align=\"center\">\n",
    "<img src=\"media/profile_hf_llama_bf16.png\" width=\"100%\">\n",
    "    <figcaption> Fig 1: Profile of HF Llama baseline implementation. (Top) A cross-section of the CPU/GPU activity showing a single transformer layer (mainly \"SelfAttention\" and \"MLP\" modules) forward. (Middle) An emphasized cross-section of a portion of the \"SelfAttention\" module that shows the \"Q\", \"K\" and \"V\" projection operations (basically `nn.linear` layers). (Bottom) A further emphasized cross-section of only the \"Q\" projection operation which shows the corresponding CPU and GPU activity for a single `nn.Linear` layer. </figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "<figure align=\"center\">\n",
    "<img src=\"media/profile_hf_llama_fp8.png\" width=\"100%\">\n",
    "    <figcaption> Fig 1: Profile of HF Llama with \"Improvement 1\" (replacing `nn.Linear` layers with TE's `Linear` layers) implementation. (Top) A cross-section of the CPU/GPU activity showing a single transformer layer (mainly \"MultiheadAttention\" and \"LayerNormMLP\" modules) forward. (Middle) An emphasized cross-section of a portion of the \"MultiheadAttention\" module that shows the Q, K and V projection operations (basically TE's `Linear` layers). (Bottom) A further emphasized cross-section of only the Q projection operation which shows the CPU and GPU activity for a single TE's `Linear` layer.</figcaption>\n",
    "</figure>\n",
    "\n",
    "<div class=\"alert alert-light\">\n",
    "\n",
    "<b>Insight</b>\n",
    "    \n",
    "In the profiles above, whenever the GPU activity is absent, it generally indicates that the GPU is waiting for CPU to dispatch a kernel i.e. the GPU is idle and waiting for more work. In general, we'd want the GPU to be active as much as possible and wait less for the CPU.\n",
    "    \n",
    "One thing clearly noticeable is that the GPU kernels in the \"baseline\" implementation occupy GPU more of the time than the \"Improvement 1\" (replacing `nn.Linear` with TE's `Linear`) implementation. Let's dig into why that is the case.\n",
    "\n",
    "</div>\n",
    "\n",
    "To simplify the information from the above profiles, consider the following table that compares the two implementations:\n",
    "\n",
    "|                                                     |              |                        |              |                   | Baseline (microseconds) | Improvement 1 (replace `nn.Linear` with TE's `Linear`)(microseconds) | Speedup |\n",
    "|-----------------------------------------------------|--------------|------------------------|--------------|-------------------|-------------------------|----------------------------------------------------------------------|---------|\n",
    "| Single transformer layer forward (\"attn\" and \"mlp\") |              |                        |              |                   | 2443                    | 6299                                                                 | -       |\n",
    "|                                                     | \"attn\" layer |                        |              |                   | 1470                    | 3842                                                                 | -       |\n",
    "|                                                     |              | Q, K and V projections |              |                   | 326                     | 2027                                                                 | -       |\n",
    "|                                                     |              |                        | Q projection |                   | 117                     | 1001                                                                 | -       |\n",
    "|                                                     |              |                        |              | Amax/Scale update | -                       | 72                                                                   | -       |\n",
    "|                                                     |              |                        |              | Buffer allocation | -                       | 49                                                                   | -       |\n",
    "|                                                     |              |                        |              | Cast+Transpose    | -                       | 35 (10 + 25)                                                         | -       |\n",
    "|                                                     |              |                        |              | MatMul            | 106                     | 52                                                                   | 2.03x   |\n",
    "|                                                     | \"mlp\" layer  |                        |              |                   | 790                     | 2057                                                                 | -       |\n",
    "\n",
    "\n",
    "Now let's make a few observations:\n",
    "\n",
    "1. For a single transformer layer, Improvement 1 implementation (`nn.Linear` layers replaced with TE's `Linear` layers and with FP8 precision) takes more time than the baseline implementation (BF16 precision). \n",
    "2. If we keep zooming in the profile to individual \"attn\" (`SelfAttention` for baseline and `MultiheadAttention` for Improvement 1) or \"mlp\" (`MLP` for baseline or `LayerNormMLP` for Improvement 1) layers, the trend is similar that Improvement 1 is slower than the baseline implementation.\n",
    "3. At its core the `Linear` layers in Improvement 1 are slower than the `nn.Linear` layers in the baseline implementation (1001 microseconds vs 117 microseconds, i.e. _slower by a factor of **8.5x**_)\n",
    "\n",
    "#### Why is TE's `Linear` slower than `nn.Linear` layer?\n",
    "\n",
    "If we look closely, TE's `Linear` layer contains more kernels for the following tasks: \n",
    "1. Amax and scale update \n",
    "2. FP8 weights and transpose buffer allotment\n",
    "3. Cast+Transpose kernels for inputs and weights (to cast BF16 inputs and weights to their FP8 counterparts)\n",
    "4. Matrix Multiplication kernel (in FP8 precision)\n",
    "\n",
    "All those kernels are pretty short for the workload for the current finetuning tutorial (`batch_size=8`, `max_seq_length=256`).\n",
    "\n",
    "Compare this to `nn.Linear` layer that contains only a Matrix Multiplication kernel (in BF16 precision\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "<b>Insight</b>\n",
    "    \n",
    "In the Improvement 1, the Matrix Multiplication operation in FP8 precision itself is faster than the Matrix Multiplication in BF16 precision in the baseline implementation!\n",
    "    \n",
    "What if we could force the GPU to spend more time in Matrix Multiplication?\n",
    "\n",
    "</div>\n",
    "\n",
    "#### How to make TE's `Linear` faster than `nn.Linear` layer?\n",
    "\n",
    "As we noted earlier, the workload for the current finetuning tutorial (`batch_size=8`, `max_seq_length=256`) is a bit short and therefore isn't able to fully utilize the capability of TE's `Linear` layers.\n",
    "\n",
    "Generally, we'd want to increase the workload so that GPU is active more of the time than the CPU.\n",
    "\n",
    "As a small experiment, let's see how the profiles look like for the \"Q\" projection operation when we increase the `batch_size` from `8` to `128`. Since this result in GPU running of memory, let's simultaneously decrease the size of our model from 32 layers to just `4` (i.e. `config=num_hidden_layers=4`).\n",
    "\n",
    "\n",
    "<figure align=\"center\">\n",
    "<img src=\"media/profile_hf_llama_bf16_bs_128.png\" width=\"100%\">\n",
    "    <figcaption> Fig 1: (baseline implementation) Profile of the \"Q\" projection, especially the `nn.Linear` layer when batch_size=128 and num_hidden_layers=4. </figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "<figure align=\"center\">\n",
    "<img src=\"media/profile_hf_llama_fp8_bs_128.png\" width=\"100%\">\n",
    "    <figcaption> Fig 1: (Improvement 1 implementation) Profile of the \"Q\" projection, especially the TE's `Linear` layer when  batch_size=128 and num_hidden_layers=4. </figcaption>\n",
    "</figure>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa8d5b8",
   "metadata": {},
   "source": [
    "## From \"Transformer\" to \"Llama\" \n",
    "\n",
    "<figure align=\"center\">\n",
    "<img src=\"media/transformer_llama.png\" width=\"50%\">\n",
    "    <figcaption> Fig 1: Llama visualized as a transformer. (generated with <a href=\"https://catalog.ngc.nvidia.com/orgs/nvidia/teams/ai-foundation/models/sdxl\">Nvidia's AI-foundation models</a>)</figcaption>\n",
    "</figure>\n",
    "\n",
    "A flashback:\n",
    "- 2017: [\"Attention Is All You Need\"](https://arxiv.org/abs/1706.03762) paper introduced pioneering \"Transformer\" architecture and changed the NLP field forever.\n",
    "- 2018-2020: Emergence of GPT model series that showed causal decoder architectures are great fit for pretraining, few-shot and zero-shot learning.\n",
    "- Fast forward to 2023-2024: Following GPT-3/GPT-4 success stories, researchers and companies raced to produce the next best pretrained model that could further be finetuned for application-specific use-cases. \n",
    "- One of the latest in this line of pretrained models which is also open source is Meta's [Llama 2](https://llama.meta.com/llama2) models (Large Language Model Meta AI). \n",
    "    - These models range from 7B to 65B parameters.\n",
    "    - LLaMA 2 was pretrained on 2 trillion tokens.\n",
    "\n",
    "For more information on Llama 2 consider reading the [Huggingface tutorial](https://huggingface.co/blog/llama2). As a quick summary, here are some of the important differences b/w the conventional decoder architecture vs Llama2:\n",
    "\n",
    "[image about decoder arch vs llama2]\n",
    "\n",
    "1. Decoder only model (causal language modeling and next word prediction)\n",
    "2. RMSNorm in place of the LayerNorm\n",
    "3. SwiGLU activation function\n",
    "4. RoPE as positional embeddings \n",
    "5. Grouped Query Attention\n",
    "6. Trained on 4K context length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a238d8a",
   "metadata": {},
   "source": [
    "## Hugging Face's `LlamaModel`\n",
    "Hugging Face provides an open-source implementation of `Llama` model in [`modeling_llama.py`](https://github.com/huggingface/transformers/blob/3d2900e829ab16757632f9dde891f1947cfc4be0/src/transformers/models/llama/modeling_llama.py#L4).\n",
    "\n",
    "Here's a block diagram that shows how Llama model is implemented in the Hugging Face repo. Notice the modular encapsulated form and `LlamaDecoderLayer` at the core of the model implementation. This core layer chunk is targeted for optimizations in a couple of the improvements later in this tutorial (Improvement 2 and Improvement 3). \n",
    "\n",
    "<figure align=\"center\">\n",
    "<img src=\"media/llama_for_causal_lm.png\" width=\"40%\">\n",
    "    <figcaption> Fig 2: Causal Llama Model Block Diagram. </figcaption>\n",
    "</figure>\n",
    "\n",
    "The above diagram translates to the following text output of the model in pytorch. Notice that the core of the model has 32 `LlamaDecoderLayer`s. \n",
    "\n",
    "```\n",
    "LlamaForCausalLM(\n",
    "  (model): LlamaModel(\n",
    "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
    "    (layers): ModuleList(\n",
    "      (0-31): 32 x LlamaDecoderLayer(\n",
    "        (self_attn): LlamaFlashAttention2(\n",
    "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
    "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
    "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
    "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
    "          (rotary_emb): LlamaRotaryEmbedding()\n",
    "        )\n",
    "        (mlp): LlamaMLP(\n",
    "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
    "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
    "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
    "          (act_fn): SiLU()\n",
    "        )\n",
    "        (input_layernorm): LlamaRMSNorm()\n",
    "        (post_attention_layernorm): LlamaRMSNorm()\n",
    "      )\n",
    "    )\n",
    "    (norm): LlamaRMSNorm()\n",
    "  )\n",
    "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
    ")\n",
    "```\n",
    "\n",
    "#### HF `LlamaDecoderLayer`\n",
    "\n",
    "Let's take a closer look at `LlamaDecoderLayer`. It's composed of `input_layernorm`, `self_attn`, `post_attention_layernorm` and `mlp` modules. Each module has associated weights as shown in the diagram.\n",
    "\n",
    "<figure align=\"center\">\n",
    "<img src=\"media/llama_zoom.png\" width=\"70%\">\n",
    "    <figcaption> Fig 3: Causal Llama Model Block Diagram (<a href=\"https://github.com/huggingface/transformers/blob/e770f0316d2a9b787c9d1440f204fcb65e176682/src/transformers/models/llama/modeling_llama.py#L695\">LlamaDecoderLayer</a>). </figcaption>\n",
    "</figure>\n",
    "\n",
    "- [ ] Explain the layers a bit (e.g. Swiglu (up_proj and gate_proj))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b61e2c",
   "metadata": {},
   "source": [
    "## [Baseline] Running HF `LlamaModel` (Precision: `BF16`)\n",
    "\n",
    "Llama2 weights are loaded into the Hugging Face native implementation `LlamaForCausalLM` (refer to [modeling_llama.py](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py)). \n",
    "\n",
    "`batch_size` is `8` and precision is `BF16`\n",
    "\n",
    "The `LlamaDecoderLayer` is left unchanged in the baseline as follows:\n",
    "\n",
    "<figure align=\"center\">\n",
    "<img src=\"media/llamadecoderlayer.png\" width=\"30%\">\n",
    "    <figcaption> Fig 4: Revisiting \"LlamaDecoderLayer\". </figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "542e65c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69939776",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54270198784"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.memory_allocated(device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e69014a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "Loading checkpoint shards:   0%|                                                                                                                                                                                                                                                                                                                                                                                                                                               | 0/2 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/_utils.py:836: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.52it/s]\n",
      "/perfhome/repos/2023/megatron/vicuna/hf/accelerate/src/accelerate/accelerator.py:387: UserWarning: `log_with=wandb` was passed but no supported trackers are currently installed.\n",
      "  warnings.warn(f\"`log_with={log_with}` was passed but no supported trackers are currently installed.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: bf16\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n",
      "  warnings.warn(\"Repo card metadata block was not found. Setting CardData to empty.\")\n",
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9846/9846 [00:00<00:00, 11108.04 examples/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 86.00 MiB. GPU 0 has a total capacity of 63.21 GiB of which 52.75 MiB is free. Process 30030 has 63.15 GiB memory in use. Of the allocated memory 60.88 GiB is allocated by PyTorch, and 1.55 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 26\u001b[0m\n\u001b[1;32m     22\u001b[0m accelerator, model, optimizer, train_dataloader, lr_scheduler \u001b[38;5;241m=\u001b[39m wrap_with_accelerator(model, hyperparams)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m## Finetune the model\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[43mfinetune_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyperparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/perfhome/repos/2023/TransformerEngine/docs/examples/te_llama/utils.py:139\u001b[0m, in \u001b[0;36mfinetune_model\u001b[0;34m(model, hyperparams, accelerator, train_dataloader, optimizer, lr_scheduler)\u001b[0m\n\u001b[1;32m    137\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m    138\u001b[0m accelerator\u001b[38;5;241m.\u001b[39mbackward(loss)\n\u001b[0;32m--> 139\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    141\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/perfhome/repos/2023/megatron/vicuna/hf/accelerate/src/accelerate/optimizer.py:149\u001b[0m, in \u001b[0;36mAcceleratedOptimizer.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accelerate_step_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 149\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator_state\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mXLA:\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_state\u001b[38;5;241m.\u001b[39mis_xla_gradients_synced \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:68\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     67\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py:187\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    174\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    176\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    177\u001b[0m         group,\n\u001b[1;32m    178\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    184\u001b[0m         state_steps,\n\u001b[1;32m    185\u001b[0m     )\n\u001b[0;32m--> 187\u001b[0m     \u001b[43madamw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py:340\u001b[0m, in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adamw\n\u001b[0;32m--> 340\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py:609\u001b[0m, in \u001b[0;36m_multi_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001b[0m\n\u001b[1;32m    607\u001b[0m     exp_avg_sq_sqrt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_foreach_sqrt(device_max_exp_avg_sqs)\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 609\u001b[0m     exp_avg_sq_sqrt \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_foreach_sqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_exp_avg_sqs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    611\u001b[0m torch\u001b[38;5;241m.\u001b[39m_foreach_div_(exp_avg_sq_sqrt, bias_correction2_sqrt)\n\u001b[1;32m    612\u001b[0m torch\u001b[38;5;241m.\u001b[39m_foreach_add_(exp_avg_sq_sqrt, eps)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 86.00 MiB. GPU 0 has a total capacity of 63.21 GiB of which 52.75 MiB is free. Process 30030 has 63.15 GiB memory in use. Of the allocated memory 60.88 GiB is allocated by PyTorch, and 1.55 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# Minimize the bloat by wrapping all the imports in a function and return\n",
    "from utils import *\n",
    "\n",
    "\n",
    "## Default hyperparams, also defined in `utils.py` in class `Hyperparameters`\n",
    "# hyperparams.dataset_name = \"timdettmers/openassistant-guanaco\"\n",
    "# hyperparams.dataset_text_field = \"text\"\n",
    "# hyperparams.learning_rate = 1.41e-5\n",
    "# hyperparams.batch_size = 8\n",
    "# hyperparams.max_seq_length = 256\n",
    "# hyperparams.gradient_accumulation_steps = 1\n",
    "# hyperparams.num_training_steps=10\n",
    "\n",
    "\n",
    "## !!! `model_name` attr must point to the location of the model weights !!!\n",
    "hyperparams.model_name = \"/bei/datasets/llama-7bf-hf/\" # <== Add model weight location here\n",
    "hyperparams.mixed_precision = \"bf16\"\n",
    "\n",
    "\n",
    "## Init the model and accelerator wrapper\n",
    "model = init_baseline_model(hyperparams)\n",
    "accelerator, model, optimizer, train_dataloader, lr_scheduler = wrap_with_accelerator(model, hyperparams)\n",
    "\n",
    "\n",
    "## Finetune the model\n",
    "finetune_model(model, hyperparams, accelerator, train_dataloader, optimizer, lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41a14b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: loss: 1.742400050163269, batch shape: torch.Size([8, 256]), peak gpu mem: 25.39 GB\n",
      "Step 0 time 0.7563114166259766\n",
      "Step 1: loss: 2.0216784477233887, batch shape: torch.Size([8, 256]), peak gpu mem: 63.09 GB\n",
      "Step 1 time 0.2901787757873535\n",
      "Step 2: loss: 2.2615396976470947, batch shape: torch.Size([8, 256]), peak gpu mem: 63.09 GB\n",
      "Step 2 time 0.2868986129760742\n",
      "Step 3: loss: 2.092174530029297, batch shape: torch.Size([8, 256]), peak gpu mem: 63.09 GB\n",
      "Step 3 time 0.289764404296875\n",
      "Step 4: loss: 2.0941758155822754, batch shape: torch.Size([8, 256]), peak gpu mem: 63.09 GB\n",
      "Step 4 time 0.28885555267333984\n",
      "Step 5: loss: 2.010141372680664, batch shape: torch.Size([8, 256]), peak gpu mem: 63.09 GB\n",
      "Step 5 time 0.287872314453125\n",
      "Step 6: loss: 2.3474209308624268, batch shape: torch.Size([8, 256]), peak gpu mem: 63.09 GB\n",
      "Step 6 time 0.2875847816467285\n",
      "Step 7: loss: 1.8907099962234497, batch shape: torch.Size([8, 256]), peak gpu mem: 63.09 GB\n",
      "Step 7 time 0.28859663009643555\n",
      "Step 8: loss: 1.9204421043395996, batch shape: torch.Size([8, 256]), peak gpu mem: 63.09 GB\n",
      "Step 8 time 0.289294958114624\n",
      "Step 9: loss: 1.9386579990386963, batch shape: torch.Size([8, 256]), peak gpu mem: 63.09 GB\n",
      "Step 9 time 0.28856849670410156\n"
     ]
    }
   ],
   "source": [
    "# Fine-tune the model\n",
    "model.train()\n",
    "completed_steps = 0\n",
    "total_loss = 0\n",
    "optimizer.zero_grad()\n",
    "\n",
    "for _ in range(10):\n",
    "    if completed_steps >= num_training_steps:\n",
    "        break\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        start_time = time.time()\n",
    "        with accelerator.accumulate(model):\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            print(f\"Step {step}: loss: {loss.item()}, batch shape: {batch['input_ids'].shape}, peak gpu mem: {torch.cuda.max_memory_allocated() / 1024**3:.2f} GB\")\n",
    "            total_loss += loss.detach().float()\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if accelerator.sync_gradients:\n",
    "            completed_steps += 1\n",
    "\n",
    "        end_time = time.time()\n",
    "        total_time = end_time - start_time\n",
    "        print(f\"Step {step} time {total_time}\")\n",
    "        accelerator.log({\"batch_time\": total_time, \"input_ids\": batch[\"input_ids\"].cpu().numpy(), \"attention_mask\": batch[\"attention_mask\"].cpu().numpy()})\n",
    "        start_time = end_time\n",
    "\n",
    "        if completed_steps >= num_training_steps:\n",
    "            break\n",
    "\n",
    "accelerator.end_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d042b9b",
   "metadata": {},
   "source": [
    "| Models                                                      | Precision | Step Time (or ms per batch) | Speedup (over baseline) |\n",
    "|-------------------------------------------------------------|-----------|-----------------------------|-------------------------|\n",
    "| HF (baseline)                                               | BF16      | 288                         | 1                     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160960a1",
   "metadata": {},
   "source": [
    "## [Improvement 1] Replace `nn.Linear` with TE's `Linear` layers (Precision: `FP8`)\n",
    "\n",
    "[Hugging Face accelerate](https://github.com/huggingface/accelerate) provides a [`convert_model`](https://github.com/huggingface/accelerate/blob/97d2168e5953fe7373a06c69c02c5a00a84d5344/src/accelerate/utils/transformer_engine.py#L24) method to replace `torch.nn.Linear` layers with `transformer_engine.pytorch.module.Linear` layers. The following diagram illustrates this visually.\n",
    "\n",
    "<figure align=\"center\">\n",
    "<img src=\"media/llamadecoderlayer_replace_with_telinear.png\" width=\"70%\">\n",
    "    <figcaption> Fig 5: Replacing \"nn.Linear\" with \"TE.Linear\". </figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "This is the most straightforward way to use TransformerEngine's `FP8` precision during training/finetuning for HF Llama model. Notice that the entire `LlamaDecoderLayer` is mostly left unchanged, it's only the `nn.Linear` layers that get replaced with `TE.Linear` layers. After replacing, these layers can be run in `FP8` precision.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8a10a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/_utils.py:836: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.24s/it]\n"
     ]
    }
   ],
   "source": [
    "# Minimize the bloat by wrapping all the imports in a function and return\n",
    "from utils import *\n",
    "\n",
    "\n",
    "## Default hyperparams, also defined in `utils.py` in class `Hyperparameters`\n",
    "# hyperparams.dataset_name = \"timdettmers/openassistant-guanaco\"\n",
    "# hyperparams.dataset_text_field = \"text\"\n",
    "# hyperparams.learning_rate = 1.41e-5\n",
    "# hyperparams.batch_size = 8\n",
    "# hyperparams.max_seq_length = 256\n",
    "# hyperparams.gradient_accumulation_steps = 1\n",
    "# hyperparams.num_training_steps=10\n",
    "\n",
    "\n",
    "## !!! `model_name` attr must point to the location of the model weights !!!\n",
    "hyperparams.model_name = \"/ckpt/llama-7bf-hf/\" # <== Add model weight location here\n",
    "hyperparams.mixed_precision = \"fp8\"\n",
    "\n",
    "\n",
    "## Init the model and accelerator wrapper\n",
    "model = init_baseline_model(hyperparams)\n",
    "accelerator, model, optimizer, train_dataloader, lr_scheduler = wrap_with_accelerator(model, hyperparams)\n",
    "\n",
    "\n",
    "## Finetune the model\n",
    "finetune_model(model, hyperparams, accelerator, train_dataloader, optimizer, lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7142363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: loss: 1.7408778667449951, batch shape: torch.Size([8, 256]), peak gpu mem: 30.95 GB\n",
      "Step 0 time 1.494196891784668\n",
      "Step 1: loss: 2.0046780109405518, batch shape: torch.Size([8, 256]), peak gpu mem: 68.52 GB\n",
      "Step 1 time 0.926837682723999\n",
      "Step 2: loss: 2.2610206604003906, batch shape: torch.Size([8, 256]), peak gpu mem: 68.52 GB\n",
      "Step 2 time 0.30959415435791016\n",
      "Step 3: loss: 2.0951247215270996, batch shape: torch.Size([8, 256]), peak gpu mem: 68.55 GB\n",
      "Step 3 time 0.3102715015411377\n",
      "Step 4: loss: 2.0994796752929688, batch shape: torch.Size([8, 256]), peak gpu mem: 68.55 GB\n",
      "Step 4 time 0.30820536613464355\n",
      "Step 5: loss: 2.005664587020874, batch shape: torch.Size([8, 256]), peak gpu mem: 68.55 GB\n",
      "Step 5 time 0.3083922863006592\n",
      "Step 6: loss: 2.3457205295562744, batch shape: torch.Size([8, 256]), peak gpu mem: 68.55 GB\n",
      "Step 6 time 0.30889034271240234\n",
      "Step 7: loss: 1.8973335027694702, batch shape: torch.Size([8, 256]), peak gpu mem: 68.55 GB\n",
      "Step 7 time 0.30847978591918945\n",
      "Step 8: loss: 1.9132524728775024, batch shape: torch.Size([8, 256]), peak gpu mem: 68.55 GB\n",
      "Step 8 time 0.30858278274536133\n",
      "Step 9: loss: 1.9362869262695312, batch shape: torch.Size([8, 256]), peak gpu mem: 68.55 GB\n",
      "Step 9 time 0.30936670303344727\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model.train()\n",
    "completed_steps = 0\n",
    "total_loss = 0\n",
    "optimizer.zero_grad()\n",
    "\n",
    "for _ in range(10):\n",
    "    if completed_steps >= num_training_steps:\n",
    "        break\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        start_time = time.time()\n",
    "        with accelerator.accumulate(model):\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            print(f\"Step {step}: loss: {loss.item()}, batch shape: {batch['input_ids'].shape}, peak gpu mem: {torch.cuda.max_memory_allocated() / 1024**3:.2f} GB\")\n",
    "            total_loss += loss.detach().float()\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if accelerator.sync_gradients:\n",
    "            completed_steps += 1\n",
    "\n",
    "        end_time = time.time()\n",
    "        total_time = end_time - start_time\n",
    "        print(f\"Step {step} time {total_time}\")\n",
    "        accelerator.log({\"batch_time\": total_time, \"input_ids\": batch[\"input_ids\"].cpu().numpy(), \"attention_mask\": batch[\"attention_mask\"].cpu().numpy()})\n",
    "        start_time = end_time\n",
    "\n",
    "        if completed_steps >= num_training_steps:\n",
    "            break\n",
    "\n",
    "accelerator.end_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874fefec",
   "metadata": {},
   "source": [
    "Based on the above run, the performance of the baseline HF Llama 2 implementation is as follows:\n",
    "\n",
    "| Models                                                      | Precision | Step Time (or ms per batch) | Speedup (over baseline) |\n",
    "|-------------------------------------------------------------|-----------|-----------------------------|-------------------------|\n",
    "| HF (baseline)                                               | BF16      | 288                         | 1                       |\n",
    "| HF (replace `nn.Linear` with `TE.Linear`)                   | FP8       | 307                         | _**0.94**_                 \n",
    "\n",
    "The performance with TE `Linear` layers has actually decreased. Let's try to understand the reason of the slow down. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbabb16",
   "metadata": {},
   "source": [
    "### Understanding the aberration with Improvement 2 (CPU Overheads)\n",
    "\n",
    "\n",
    "<figure align=\"center\">\n",
    "<img src=\"media/profile_hf_llama_bf16.png\" width=\"100%\">\n",
    "    <figcaption> Fig 1: Profile of HF Llama baseline implementation. (Top) A cross-section of the CPU/GPU activity showing a single transformer layer (mainly \"SelfAttention\" and \"MLP\" modules) forward. (Middle) An emphasized cross-section of a portion of the \"SelfAttention\" module that shows the \"Q\", \"K\" and \"V\" projection operations (basically `nn.linear` layers). (Bottom) A further emphasized cross-section of only the \"Q\" projection operation which shows the corresponding CPU and GPU activity for a single `nn.Linear` layer. </figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "<figure align=\"center\">\n",
    "<img src=\"media/profile_hf_llama_fp8.png\" width=\"100%\">\n",
    "    <figcaption> Fig 1: Profile of HF Llama with \"Improvement 1\" (replacing `nn.Linear` layers with TE's `Linear` layers) implementation. (Top) A cross-section of the CPU/GPU activity showing a single transformer layer (mainly \"MultiheadAttention\" and \"LayerNormMLP\" modules) forward. (Middle) An emphasized cross-section of a portion of the \"MultiheadAttention\" module that shows the Q, K and V projection operations (basically TE's `Linear` layers). (Bottom) A further emphasized cross-section of only the Q projection operation which shows the CPU and GPU activity for a single TE's `Linear` layer.</figcaption>\n",
    "</figure>\n",
    "\n",
    "<div class=\"alert alert-light\">\n",
    "\n",
    "<b>Insight</b>\n",
    "    \n",
    "In the profiles above, whenever the GPU activity is absent, it generally indicates that the GPU is waiting for CPU to dispatch a kernel i.e. the GPU is idle and waiting for more work. In general, we'd want the GPU to be active as much as possible and wait less for the CPU.\n",
    "    \n",
    "One thing clearly noticeable is that the GPU kernels in the \"baseline\" implementation occupy GPU more of the time than the \"Improvement 1\" (replacing `nn.Linear` with TE's `Linear`) implementation. Let's dig into why that is the case.\n",
    "\n",
    "</div>\n",
    "\n",
    "To simplify the information from the above profiles, consider the following table that compares the two implementations:\n",
    "\n",
    "|                                                     |              |                        |              |                   | Baseline (microseconds) | Improvement 1 (replace `nn.Linear` with TE's `Linear`)(microseconds) | Speedup |\n",
    "|-----------------------------------------------------|--------------|------------------------|--------------|-------------------|-------------------------|----------------------------------------------------------------------|---------|\n",
    "| Single transformer layer forward (\"attn\" and \"mlp\") |              |                        |              |                   | 2443                    | 6299                                                                 | -       |\n",
    "|                                                     | \"attn\" layer |                        |              |                   | 1470                    | 3842                                                                 | -       |\n",
    "|                                                     |              | Q, K and V projections |              |                   | 326                     | 2027                                                                 | -       |\n",
    "|                                                     |              |                        | Q projection |                   | 117                     | 1001                                                                 | -       |\n",
    "|                                                     |              |                        |              | Amax/Scale update | -                       | 72                                                                   | -       |\n",
    "|                                                     |              |                        |              | Buffer allocation | -                       | 49                                                                   | -       |\n",
    "|                                                     |              |                        |              | Cast+Transpose    | -                       | 35 (10 + 25)                                                         | -       |\n",
    "|                                                     |              |                        |              | MatMul            | 106                     | 52                                                                   | 2.03x   |\n",
    "|                                                     | \"mlp\" layer  |                        |              |                   | 790                     | 2057                                                                 | -       |\n",
    "\n",
    "\n",
    "Now let's make a few observations:\n",
    "\n",
    "1. For a single transformer layer, Improvement 1 implementation (`nn.Linear` layers replaced with TE's `Linear` layers and with FP8 precision) takes more time than the baseline implementation (BF16 precision). \n",
    "2. If we keep zooming in the profile to individual \"attn\" (`SelfAttention` for baseline and `MultiheadAttention` for Improvement 1) or \"mlp\" (`MLP` for baseline or `LayerNormMLP` for Improvement 1) layers, the trend is similar that Improvement 1 is slower than the baseline implementation.\n",
    "3. At its core the `Linear` layers in Improvement 1 are slower than the `nn.Linear` layers in the baseline implementation (1001 microseconds vs 117 microseconds, i.e. _slower by a factor of **8.5x**_)\n",
    "\n",
    "#### Why is TE's `Linear` slower than `nn.Linear` layer?\n",
    "\n",
    "If we look closely, TE's `Linear` layer contains more kernels for the following tasks: \n",
    "1. Amax and scale update \n",
    "2. FP8 weights and transpose buffer allotment\n",
    "3. Cast+Transpose kernels for inputs and weights (to cast BF16 inputs and weights to their FP8 counterparts)\n",
    "4. Matrix Multiplication kernel (in FP8 precision)\n",
    "\n",
    "While the GPU is idle, it's usually waiting for CPU to finish doing its work and dispatch a kernel that can run on the GPU! Further, all those kernels are pretty short for the workload for the current finetuning tutorial (`batch_size=8`, `max_seq_length=256`). Therefore, overall the time taken by the TE's `Linear` layer is **1001 microseconds**. \n",
    "\n",
    "Compare this to `nn.Linear` layer (in Fig: ???) which contains only a Matrix Multiplication kernel (in BF16 precision). Almost all of the work inside the linear layer is running that kernel. Further, as the GPU doesn't have to wait for the CPU for more kernels, the `nn.Linear` layer itself takes less time to run, only **117 microseconds**, almost a tenth of the fraction of TE's `Linear` layer. \n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "<b>Insight</b>\n",
    "    \n",
    "Note that in Improvement 1, the Matrix Multiplication operation in FP8 precision itself is faster than the Matrix Multiplication in BF16 precision in the baseline implementation!\n",
    "    \n",
    "What if we could force the GPU to spend more time in Matrix Multiplication?\n",
    "\n",
    "</div>\n",
    "\n",
    "#### How to make TE's `Linear` faster than `nn.Linear` layer?\n",
    "\n",
    "As we noted earlier, the workload for the current finetuning tutorial (`batch_size=8`, `max_seq_length=256`) is a bit short and therefore isn't able to fully utilize the capability of TE's `Linear` layers.\n",
    "\n",
    "Generally, we'd want to increase the workload so that GPU is active more of the time than the CPU.\n",
    "\n",
    "As a small experiment, let's see how the profiles look like for the \"Q\" projection operation when we increase the `batch_size` from `8` to `128`. Since this result in GPU running of memory, let's simultaneously decrease the size of our model from 32 layers to just `4` (i.e. `config=num_hidden_layers=4`).\n",
    "\n",
    "\n",
    "<figure align=\"center\">\n",
    "<img src=\"media/profile_hf_llama_bf16_bs_128.png\" width=\"100%\">\n",
    "    <figcaption> Fig 1: (baseline implementation) Profile of the \"Q\" projection, especially the `nn.Linear` layer when batch_size=128 and num_hidden_layers=4. </figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "<figure align=\"center\">\n",
    "<img src=\"media/profile_hf_llama_fp8_bs_128.png\" width=\"100%\">\n",
    "    <figcaption> Fig 1: (Improvement 1 implementation) Profile of the \"Q\" projection, especially the TE's `Linear` layer when  batch_size=128 and num_hidden_layers=4. </figcaption>\n",
    "</figure>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdd32d7",
   "metadata": {},
   "source": [
    "### Understanding the aberration with Improvement 2 (CPU Overheads)\n",
    "\n",
    "TransformerEngine's `Linear` layer has to do some extra work (in python, so on CPU) before it can issue `FP8` version of GEMMs (matrix-multiplies from the linear layers) to the GPU.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "2. So, if batch-size and/or sequence-length are small enough, the GEMM will be smaller. The GEMMs on GPU therefore usually finish early than the CPU can issue them in succession. In this case, we say the the workload is CPU bound. Ideally we'd want the workload to be GPU bound, i.e. it should spend more time in the GPU to fully utilize its capability.\n",
    "3. We can verify this by looking the profiles of the two workloads (using Nvidia Nsight Systems visualizer).\n",
    "    - For the baseline case (with `nn.Linear` layers):\n",
    "        ![baseline profile](media/baseline.png \"baseline\")\n",
    "        \n",
    "    - Improvement 1 - when `nn.Linear`s are replaced with `TE.Linear`s:\n",
    "        ![replace linears](media/replace_nnlinear_with_telinear.png \"replace linears\")\n",
    "        \n",
    "    - As we can see, the GPU is busy comparatively less in the second case, compared to the first. CPU overhead is not letting \n",
    "\n",
    "But How do we alleviate this issue? \n",
    "\n",
    "To fully utilize the TransformerEngine's capabilities, the workload should be larger. In this present tutorial, the batch-size is 8 which is fine but sequence-length is 256 which is pretty low. If the sequence-length is large enough, even this case could be faster than the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f64542",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>Jupyter.notebook.kernel.restart()</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\") #Restart the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a33225da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9888b583",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams.model_name = \"/bei/datasets/llama-7bf-hf\"\n",
    "config = AutoConfig.from_pretrained(hyperparams.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb1faccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.num_hidden_layers = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fce08b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "Loading checkpoint shards:   0%|                                                                                                                                                                                                                                                                                                                                                                                                                                               | 0/2 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/_utils.py:836: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  3.91it/s]\n",
      "Some weights of the model checkpoint at /bei/datasets/llama-7bf-hf were not used when initializing LlamaForCausalLM: ['model.layers.10.input_layernorm.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.10.post_attention_layernorm.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.11.input_layernorm.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.11.post_attention_layernorm.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.12.input_layernorm.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.12.post_attention_layernorm.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.13.input_layernorm.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.13.post_attention_layernorm.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.14.input_layernorm.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.14.post_attention_layernorm.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.15.input_layernorm.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.15.post_attention_layernorm.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.16.input_layernorm.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.16.post_attention_layernorm.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.17.input_layernorm.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.17.post_attention_layernorm.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.18.input_layernorm.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.18.post_attention_layernorm.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.19.input_layernorm.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.19.post_attention_layernorm.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.20.input_layernorm.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.20.post_attention_layernorm.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.21.input_layernorm.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.21.post_attention_layernorm.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.22.input_layernorm.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.22.post_attention_layernorm.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.23.input_layernorm.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.23.post_attention_layernorm.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.24.input_layernorm.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.24.post_attention_layernorm.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.25.input_layernorm.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.25.post_attention_layernorm.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.26.input_layernorm.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.26.post_attention_layernorm.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.27.input_layernorm.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.27.post_attention_layernorm.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.28.input_layernorm.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.28.post_attention_layernorm.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.29.input_layernorm.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.29.post_attention_layernorm.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.30.input_layernorm.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.30.post_attention_layernorm.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.31.input_layernorm.weight', 'model.layers.31.mlp.down_proj.weight', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.31.mlp.up_proj.weight', 'model.layers.31.post_attention_layernorm.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.8.input_layernorm.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.8.post_attention_layernorm.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.9.input_layernorm.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.9.post_attention_layernorm.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.v_proj.weight']\n",
      "- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# make sure to use flash_attention to do iso comparison with TELlamaModel\n",
    "config._attn_implementation = \"flash_attention_2\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    hyperparams.model_name,\n",
    "    config=config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "# Needed for the cases when using TELlamaForCausalLM. So adding here for 1:1 comparison\n",
    "model.config.use_cache=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94356692",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams.max_seq_length = 400\n",
    "hyperparams.mixed_precision = \"fp8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b98a0363",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/perfhome/repos/2023/megatron/vicuna/hf/accelerate/src/accelerate/accelerator.py:387: UserWarning: `log_with=wandb` was passed but no supported trackers are currently installed.\n",
      "  warnings.warn(f\"`log_with={log_with}` was passed but no supported trackers are currently installed.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: fp8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n",
      "  warnings.warn(\"Repo card metadata block was not found. Setting CardData to empty.\")\n",
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9846/9846 [00:00<00:00, 10146.56 examples/s]\n"
     ]
    }
   ],
   "source": [
    "accelerator, model, optimizer, train_dataloader, lr_scheduler = wrap_with_accelerator(model, hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c614a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: loss: 9.768940925598145 time: 0.5495994091033936 batch_size torch.Size([8, 400])\n",
      "Step 1: loss: 10.110845565795898 time: 0.10734415054321289 batch_size torch.Size([8, 400])\n",
      "Step 2: loss: 10.416016578674316 time: 0.10294985771179199 batch_size torch.Size([8, 400])\n",
      "Step 3: loss: 10.218790054321289 time: 0.10591721534729004 batch_size torch.Size([8, 400])\n",
      "Step 4: loss: 10.144618034362793 time: 0.10585546493530273 batch_size torch.Size([8, 400])\n",
      "Step 5: loss: 10.026362419128418 time: 0.10659241676330566 batch_size torch.Size([8, 400])\n",
      "Step 6: loss: 10.259310722351074 time: 0.10713028907775879 batch_size torch.Size([8, 400])\n",
      "Step 7: loss: 9.968159675598145 time: 0.10583972930908203 batch_size torch.Size([8, 400])\n",
      "Step 8: loss: 9.914514541625977 time: 0.10669207572937012 batch_size torch.Size([8, 400])\n",
      "Step 9: loss: 9.677173614501953 time: 0.10484886169433594 batch_size torch.Size([8, 400])\n"
     ]
    }
   ],
   "source": [
    "finetune_model(model, hyperparams, accelerator, train_dataloader, optimizer, lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87d4fce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: loss: 9.783175468444824 time: 1.321913242340088 batch_size torch.Size([8, 400])\n",
      "Step 1: loss: 10.131692886352539 time: 0.7767603397369385 batch_size torch.Size([8, 400])\n",
      "Step 2: loss: 10.436424255371094 time: 0.09566235542297363 batch_size torch.Size([8, 400])\n",
      "Step 3: loss: 10.226944923400879 time: 0.09562826156616211 batch_size torch.Size([8, 400])\n",
      "Step 4: loss: 10.1524658203125 time: 0.09576177597045898 batch_size torch.Size([8, 400])\n",
      "Step 5: loss: 10.031890869140625 time: 0.09409642219543457 batch_size torch.Size([8, 400])\n",
      "Step 6: loss: 10.274042129516602 time: 0.09509897232055664 batch_size torch.Size([8, 400])\n",
      "Step 7: loss: 9.964439392089844 time: 0.10423588752746582 batch_size torch.Size([8, 400])\n",
      "Step 8: loss: 9.933443069458008 time: 0.10542511940002441 batch_size torch.Size([8, 400])\n",
      "Step 9: loss: 9.695547103881836 time: 0.10503625869750977 batch_size torch.Size([8, 400])\n"
     ]
    }
   ],
   "source": [
    "finetune_model(model, hyperparams, accelerator, train_dataloader, optimizer, lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c4b13f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = enumerate(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c43ef688",
   "metadata": {},
   "outputs": [],
   "source": [
    "step, batch = next(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b1bb096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 400])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ecb6450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 400])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['attention_mask'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7edac0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['attention_mask'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98db33fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'labels'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "428d189f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3: loss: 10.114119529724121 time: 0.14879465103149414 batch_size torch.Size([8, 624])\n",
      "Step 4: loss: 10.01146125793457 time: 0.20577788352966309 batch_size torch.Size([8, 1104])\n",
      "Step 5: loss: 9.758525848388672 time: 0.2152695655822754 batch_size torch.Size([8, 1200])\n",
      "Step 6: loss: 10.310785293579102 time: 0.19851088523864746 batch_size torch.Size([8, 1072])\n",
      "Step 7: loss: 9.95548152923584 time: 0.10099005699157715 batch_size torch.Size([8, 448])\n",
      "Step 8: loss: 9.752256393432617 time: 0.16744017601013184 batch_size torch.Size([8, 864])\n",
      "Step 9: loss: 9.563212394714355 time: 0.14487099647521973 batch_size torch.Size([8, 704])\n",
      "Step 10: loss: 10.125089645385742 time: 0.13337016105651855 batch_size torch.Size([8, 624])\n",
      "Step 11: loss: 9.681853294372559 time: 0.1406705379486084 batch_size torch.Size([8, 672])\n",
      "Step 12: loss: 10.049918174743652 time: 0.17294764518737793 batch_size torch.Size([8, 928])\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "total_loss = 0\n",
    "optimizer.zero_grad()\n",
    "# train_dataloader = enumerate(train_dataloader)\n",
    "\n",
    "for _ in range(hyperparams.num_training_steps):\n",
    "    step, batch = next(train_dataloader)\n",
    "    start_time = time.time()\n",
    "    with accelerator.accumulate(model):\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.detach().float()\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    print(f\"Step {step}: loss: {loss.item()} time: {total_time} batch_size {batch['input_ids'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34589af3",
   "metadata": {},
   "source": [
    "## [Improvement 2] Replace HF's `LlamaDecoderLayer` with TE's `TransformerLayer` (Precision: `BF16`)\n",
    "\n",
    "In addition to basic layers like `Linear` and `LayerNorm`, TransformerEngine offers larger modules like `MultiheadAttention` (combines \"LayerNorm\" and \"Self Attention\") and `LayerNormMLP` (combines \"LayerNorm\" and \"MLP\") that could replace their counterparts in the `LlamaDecoderLayer` and potentially provide more speedup. Further, TransformerEngine also offers a full `TransformerLayer` (which further combines `MultiheadAttention` and `LayerNormMLP` layers) which could be substituted for `LlamaDecoderLayer` (with careful mapping of the weights since the name of the weights are different for those two layers). Let's take a closer look at TransformerEngine's `TransformerLayer`. \n",
    "\n",
    "### TransformerEngine's `TransformerLayer`\n",
    "\n",
    "At a higher level, TE's `TransformerLayer` could be visualized as an apt replacement for the `LlamaDecoderLayer`. But the internals of the `TransformerLayer` are organized a bit differently. \n",
    "\n",
    "<figure align=\"center\">\n",
    "<img src=\"media/tellamadecoderlayer.png\" width=\"30%\">\n",
    "    <figcaption> Fig 6: TransformerEngine's `TransformerLayer` </figcaption>\n",
    "</figure>\n",
    "\n",
    "Just like Hugging Face's `LlamaDecoderLayer`, TransformerEngine's `TransformerLayer` encapsulates `self_attention` (as `MultiheadAttention`) and `mlp` (as `LayerNormMLP`). A major difference is that the two `Norm`s are included in the `MultiheadAttention` and `LayerNormMLP` layers as shown in the following output prompt:\n",
    "\n",
    "```\n",
    "TransformerLayer(\n",
    "    (self_attention): MultiheadAttention(\n",
    "      (layernorm_qkv): LayerNormLinear()\n",
    "      (core_attention): DotProductAttention()\n",
    "      (proj): Linear()\n",
    "    )\n",
    "    (layernorm_mlp): LayerNormMLP()\n",
    ")\n",
    "```\n",
    "\n",
    "### `TransformerLayer` options explained\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "<b>Note</b>\n",
    "    \n",
    "Here, we go over some of the options in `TransformerLayer` that are needed for the tutorial. For a complete list of options, refer the [TransformerLayer API documentation](https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/api/pytorch.html?highlight=transformerlayer#transformer_engine.pytorch.TransformerLayer).\n",
    "\n",
    "</div>\n",
    "\n",
    "In the accompanying `te_llama.py` file, `TELlamaDecoderLayer` is defined as a wrapper over TE's `TransformerLayer` with a few needed options that make `TransformerLayer` as a plug-in replacement for the HF's `LlamaDecoderLayer`.\n",
    "\n",
    "```\n",
    "class TELlamaDecoderLayer(te.pytorch.TransformerLayer):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(\n",
    "            config.hidden_size,\n",
    "            config.intermediate_size,\n",
    "            config.num_attention_heads,\n",
    "            bias=False,\n",
    "            layernorm_epsilon=config.rms_norm_eps,\n",
    "            hidden_dropout=0,\n",
    "            attention_dropout=0,\n",
    "            fuse_qkv_params=False,\n",
    "            normalization=\"RMSNorm\",\n",
    "            activation=\"swiglu\",\n",
    "            attn_input_format=\"bshd\",\n",
    "        )\n",
    "        te_rope = RotaryPositionEmbedding(config.hidden_size//config.num_attention_heads)\n",
    "        self.te_rope_emb = te_rope(max_seq_len=config.max_position_embeddings).cuda()\n",
    "```\n",
    "\n",
    "Here's a list summarizing each option briefly:\n",
    "\n",
    "1. `hidden_size`: size of each input sample.\n",
    "2. `ffn_hidden_size`: intermediate size to which samples are projected.\n",
    "3. `num_attention_heads`: number of attention heads in the transformer layer.\n",
    "4. `bias`: switch to add additive biases to the submodule layers.\n",
    "5. `layernorm_epsilon`: a value added to the denominator of layer normalization for numerical stability. Default is `1e-5`.\n",
    "6. `hidden_dropout`: dropout probability for the dropout op after FC2 layer (fully connected layer no. 2). Default is `0.1`.\n",
    "7. `attention_dropout`: dropout probability for the dropout op during multi-head attention. Default is `0.1`. \n",
    "8. `fuse_qkv_params`:  if set to True, TransformerLayer module exposes a single fused parameter for query-key-value. This enables optimizations such as QKV fusion without concatentations/splits and also enables the argument fuse_wgrad_accumulation.\n",
    "9. `normalization`: type of normalization applied. Default is `LayerNorm`.\n",
    "10. `activation`: type of activation used in the MLP block. Default is `gelu`.\n",
    "11. `attn_input_format`: controls whether the dimensions of the intermediate hidden states is 'batch first' ('bshd') or 'sequence first' ('sbhd'). `s` stands for the sequence length, `b` batch size, `h` the number of heads, `d` head size. Note that these formats are very closely related to the `qkv_format` in the `MultiHeadAttention` and `DotProductAttention` modules. \n",
    "\n",
    "\n",
    "Further, note that `RotaryPositionEmbedding` is defined as part of the TE's `TransformerLayer` itself since it expects this rope cache if RoPE is used in the model. \n",
    "\n",
    "### Comparing Hugging Face's `LlamaDecoderLayer` with TranformerEngine's `TransformerLayer`\n",
    "\n",
    "Let's revisit how `LlamaDecoderLayer`s form the core of the decoder layer stack in HF's llama implementation:\n",
    "```\n",
    "ModuleList(\n",
    "  (0-31): 32 x LlamaDecoderLayer(\n",
    "    (self_attn): LlamaAttention(\n",
    "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
    "      (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
    "      (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
    "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
    "      (rotary_emb): LlamaRotaryEmbedding()\n",
    "    )\n",
    "    (mlp): LlamaMLP(\n",
    "      (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
    "      (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
    "      (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
    "      (act_fn): SiLU()\n",
    "    )\n",
    "    (input_layernorm): LlamaRMSNorm()\n",
    "    (post_attention_layernorm): LlamaRMSNorm()\n",
    "  )\n",
    ")\n",
    "```\n",
    "\n",
    "A major portion of the Hugging Face model implementation (`LlamaDecoderLayer`) could be potentially replaced with TransformerEngine's implementation.\n",
    "\n",
    "\n",
    "### Mapping weights from HF's `LlamaDecoderLayer` to TE's `TransformerLayer`\n",
    "\n",
    "Refer the accompanying file `te_llama.py` which provides a reference to create a Llama 2 model with TE's `TransformerLayer` after replacing HF's `LlamaDecoderLayer`.\n",
    "\n",
    "Briefly, \n",
    "1. `TELlamaDecoderLayer` is added as a wrapper for `TransformerLayer`. \n",
    "```\n",
    "class TELlamaDecoderLayer(te.pytorch.TransformerLayer):\n",
    "    \"\"\"\n",
    "    Wrapper class over TE's `TransformerLayer`. This makes the wrapper very\n",
    "    similar to HF's `LlamaDecoderLayer` and easier to replace it in the code.\n",
    "\n",
    "    Args:\n",
    "        config: LlamaConfig\n",
    "        args: positional args (for compatibility with `LlamaDecoderLayer`)\n",
    "        kwargs: keyword args (for compatibility with `LlamaDecoderLayer`)\n",
    "    \"\"\"\n",
    "    def __init__(self, config, *args, **kwargs):\n",
    "        super().__init__(\n",
    "            hidden_size=config.hidden_size,\n",
    "            ffn_hidden_size=config.intermediate_size,\n",
    "            num_attention_heads=config.num_attention_heads,\n",
    "            bias=False,\n",
    "            layernorm_epsilon=config.rms_norm_eps,\n",
    "            hidden_dropout=0,\n",
    "            attention_dropout=0,\n",
    "            fuse_qkv_params=False,\n",
    "            normalization=\"RMSNorm\",\n",
    "            activation=\"swiglu\",\n",
    "            attn_input_format=\"bshd\",\n",
    "        )\n",
    "        te_rope = RotaryPositionEmbedding(config.hidden_size//config.num_attention_heads)\n",
    "        self.te_rope_emb = te_rope(max_seq_len=config.max_position_embeddings).cuda()\n",
    "\n",
    "    def forward(self,\n",
    "                hidden_states,\n",
    "                *args,\n",
    "                attention_mask,\n",
    "                **kwargs):\n",
    "        \"\"\"\n",
    "        Custom forward to make sure we only pass relevant arguments to the\n",
    "        forward pass of the `TransformerLayer`. Also, make sure the output\n",
    "        format matches the output of the HF's `LlamaDecoderLayer`.\n",
    "        \"\"\"\n",
    "        return (super().forward(hidden_states, attention_mask=attention_mask, rotary_pos_emb=self.te_rope_emb),)\n",
    "```\n",
    "\n",
    "2. Before creating a `LlamaForCausalLM`, `replace_decoder` context manager is used to monkey-patch `LlamaDecoderLayer` with `TELlamaDecoderLayer`.\n",
    "\n",
    "```\n",
    "@contextmanager\n",
    "def replace_decoder(te_decodder_cls):\n",
    "    \"\"\"\n",
    "    Replace `LlamaDecoderLayer` with custom `TELlamaDecoderLayer`.\n",
    "    \"\"\"\n",
    "    original_llama_decoder_cls = transformers.models.llama.modeling_llama.LlamaDecoderLayer\n",
    "    transformers.models.llama.modeling_llama.LlamaDecoderLayer = te_decodder_cls\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        transformers.models.llama.modeling_llama.LlamaDecoderLayer = original_llama_decoder_cls\n",
    ".\n",
    ".\n",
    ".\n",
    "class TELlamaForCausalLM:\n",
    "    \"\"\"\n",
    "    Causal LM created with `LlamaModel`. The underlying `LlamaDecoderLayer`\n",
    "    class is monkey-patched with `TELlamaDecoderLayer` class before\n",
    "    initializing the causal LM with `LlamaForCausalLM`.\n",
    "\n",
    "    Args:\n",
    "        config: LlamaConfig\n",
    "    \"\"\"\n",
    "\n",
    "    def __new__(cls, config: LlamaConfig):\n",
    "        with replace_decoder(te_decodder_cls=TELlamaDecoderLayer):\n",
    "            llama_for_causal_lm = LlamaForCausalLM(config)\n",
    "        return llama_for_causal_lm\n",
    ".\n",
    ".\n",
    ".\n",
    "```\n",
    "\n",
    "3. A custom `pretrained_from_local` method is added that copies the weights from the checkpoint (which is meant for HF Llama implementation) to the modified `TELlamaForCausalLM` by carefully mapping the weights from the `LlamaDecoderLayer` (HF) to `TransformerLayer` (TE). The method `replace_params` maps and copies apt weights from `LlamaDecoderLayer` to the `TransformerLayer`. Refer to the following diagram for more details.\n",
    "\n",
    "```\n",
    "def replace_params(hf_state_dict, te_state_dict):\n",
    "    # collect all layer prefixes to update\n",
    "    all_layer_prefixes = set()\n",
    "    for param_key in hf_state_dict.keys():\n",
    "        layer_prefix_pat = 'model.layers.\\d+.'\n",
    "        m = re.match(layer_prefix_pat, param_key)\n",
    "        if m is not None:\n",
    "            all_layer_prefixes.add(m.group())\n",
    "\n",
    "    for layer_prefix in all_layer_prefixes:\n",
    "        # When loading weights into models with less number of layers, skip the\n",
    "        # copy if the corresponding layer doesn't exist in TE model\n",
    "        if layer_prefix + 'self_attention.layernorm_qkv.layer_norm_weight' in te_state_dict:\n",
    "            te_state_dict[layer_prefix + 'self_attention.layernorm_qkv.layer_norm_weight'].data[:] = hf_state_dict[layer_prefix + 'input_layernorm.weight'].data[:]\n",
    "\n",
    "        if layer_prefix + 'self_attention.layernorm_qkv.query_weight' in te_state_dict:\n",
    "            te_state_dict[layer_prefix + 'self_attention.layernorm_qkv.query_weight'].data[:] = hf_state_dict[layer_prefix + 'self_attn.q_proj.weight'].data[:]\n",
    "\n",
    "        if layer_prefix + 'self_attention.layernorm_qkv.key_weight' in te_state_dict:\n",
    "            te_state_dict[layer_prefix + 'self_attention.layernorm_qkv.key_weight'].data[:] = hf_state_dict[layer_prefix + 'self_attn.k_proj.weight'].data[:]\n",
    "    .\n",
    "    .\n",
    "    .\n",
    "\n",
    "    return all_layer_prefixes\n",
    "```\n",
    "\n",
    "<figure align=\"center\">\n",
    "<img src=\"media/weight_swap.png\" width=\"70%\">\n",
    "    <figcaption> Fig 7: Replace `LlamaDecoderLayer` with `TransformerLayer`. </figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "After initializing the modified Llama model this way, the core decoder layers get changed to `TELlamaDecoderLayer` (wrapper around `TransformerLayer`) as shown in the following output:\n",
    "```\n",
    "ModuleList(\n",
    "  (0-31): 32 x TELlamaDecoderLayer(\n",
    "    (self_attention): MultiheadAttention(\n",
    "      (layernorm_qkv): LayerNormLinear()\n",
    "      (core_attention): DotProductAttention(\n",
    "        (flash_attention): FlashAttention()\n",
    "        (fused_attention): FusedAttention()\n",
    "        (unfused_attention): UnfusedDotProductAttention(\n",
    "          (scale_mask_softmax): FusedScaleMaskSoftmax()\n",
    "          (attention_dropout): Dropout(p=0, inplace=False)\n",
    "        )\n",
    "      )\n",
    "      (proj): Linear()\n",
    "    )\n",
    "    (layernorm_mlp): LayerNormMLP()\n",
    "  )\n",
    ")\n",
    "```\n",
    "In summary, the model gets changed as follows with a bigger chunk of the implementation (core decoder layers) coming from TransformerEngine.\n",
    "\n",
    "<figure align=\"center\">\n",
    "<img src=\"media/model_change.png\" width=\"80%\">\n",
    "    <figcaption> Fig 8: Language model after the HF's `LlamaDecoderLayer`s are replaced with TE's `TransformerLayer`s. </figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<b>Note</b>\n",
    "\n",
    "Let's first run our \"TELlama\" implementation in `BF16` precision.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf8ecc2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:836: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "# Minimize the bloat by wrapping all the imports in a function and return\n",
    "from utils import *\n",
    "\n",
    "\n",
    "## Default hyperparams, also defined in `utils.py` in class `Hyperparameters`\n",
    "# hyperparams.dataset_name = \"timdettmers/openassistant-guanaco\"\n",
    "# hyperparams.dataset_text_field = \"text\"\n",
    "# hyperparams.learning_rate = 1.41e-5\n",
    "# hyperparams.batch_size = 8\n",
    "# hyperparams.max_seq_length = 256\n",
    "# hyperparams.gradient_accumulation_steps = 1\n",
    "# hyperparams.num_training_steps=10\n",
    "\n",
    "\n",
    "## !!! `model_name` attr must point to the location of the model weights !!!\n",
    "hyperparams.model_name = \"/ckpt/llama-7bf-hf/\" # <== Add model weight location here\n",
    "hyperparams.mixed_precision = \"bf16\"\n",
    "\n",
    "\n",
    "## Init the model and accelerator wrapper\n",
    "model = init_te_llama_model(hyperparams)\n",
    "accelerator, model, optimizer, train_dataloader, lr_scheduler = wrap_with_accelerator(model, hyperparams)\n",
    "\n",
    "\n",
    "## Finetune the model\n",
    "finetune_model(model, hyperparams, accelerator, train_dataloader, optimizer, lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fe4a46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: loss: 2.285313367843628, batch shape: torch.Size([8, 256]), peak gpu mem: 24.39 GB\n",
      "Step 0 time 1.4395570755004883\n",
      "Step 1: loss: 3.433112144470215, batch shape: torch.Size([8, 256]), peak gpu mem: 63.13 GB\n",
      "Step 1 time 0.2440946102142334\n",
      "Step 2: loss: 4.062510967254639, batch shape: torch.Size([8, 256]), peak gpu mem: 63.13 GB\n",
      "Step 2 time 0.24254989624023438\n",
      "Step 3: loss: 3.3462915420532227, batch shape: torch.Size([8, 256]), peak gpu mem: 63.13 GB\n",
      "Step 3 time 0.24259161949157715\n",
      "Step 4: loss: 3.0718109607696533, batch shape: torch.Size([8, 256]), peak gpu mem: 63.13 GB\n",
      "Step 4 time 0.24408507347106934\n",
      "Step 5: loss: 3.7262790203094482, batch shape: torch.Size([8, 256]), peak gpu mem: 63.13 GB\n",
      "Step 5 time 0.24119019508361816\n",
      "Step 6: loss: 5.115753650665283, batch shape: torch.Size([8, 256]), peak gpu mem: 63.13 GB\n",
      "Step 6 time 0.24044084548950195\n",
      "Step 7: loss: 3.9757368564605713, batch shape: torch.Size([8, 256]), peak gpu mem: 63.13 GB\n",
      "Step 7 time 0.24254655838012695\n",
      "Step 8: loss: 3.293850898742676, batch shape: torch.Size([8, 256]), peak gpu mem: 63.13 GB\n",
      "Step 8 time 0.2404341697692871\n",
      "Step 9: loss: 2.613889455795288, batch shape: torch.Size([8, 256]), peak gpu mem: 63.13 GB\n",
      "Step 9 time 0.24282383918762207\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model.train()\n",
    "completed_steps = 0\n",
    "total_loss = 0\n",
    "optimizer.zero_grad()\n",
    "\n",
    "for _ in range(10):\n",
    "    if completed_steps >= num_training_steps:\n",
    "        break\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        start_time = time.time()\n",
    "        with accelerator.accumulate(model):\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            print(f\"Step {step}: loss: {loss.item()}, batch shape: {batch['input_ids'].shape}, peak gpu mem: {torch.cuda.max_memory_allocated() / 1024**3:.2f} GB\")\n",
    "            total_loss += loss.detach().float()\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if accelerator.sync_gradients:\n",
    "            completed_steps += 1\n",
    "\n",
    "        end_time = time.time()\n",
    "        total_time = end_time - start_time\n",
    "        print(f\"Step {step} time {total_time}\")\n",
    "        accelerator.log({\"batch_time\": total_time, \"input_ids\": batch[\"input_ids\"].cpu().numpy(), \"attention_mask\": batch[\"attention_mask\"].cpu().numpy()})\n",
    "        start_time = end_time\n",
    "\n",
    "        if completed_steps >= num_training_steps:\n",
    "            break\n",
    "\n",
    "accelerator.end_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714a97e0",
   "metadata": {},
   "source": [
    "| Models                                                      | Precision | Step Time (or ms per batch) | Speedup (over baseline) |\n",
    "|-------------------------------------------------------------|-----------|-----------------------------|-------------------------|\n",
    "| HF (baseline)                                               | BF16      | 288                         | 1                       |\n",
    "| HF (replace `nn.Linear` with `TE.Linear`)                   | FP8       | 307                         | 0.94                    |\n",
    "| TE (replace `LlamaDecoderLayer` with `TE.TransformerLayer`) | BF16      | 243                         | 1.19                   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1689fddf",
   "metadata": {},
   "source": [
    "## [Improvement 3] Replace HF's `LlamaDecoderLayer` with TE's `TransformerLayer` (Precision: `FP8`)\n",
    "\n",
    "Now that most of the HF Llama model implementation (`LlamaDecoderLayer`s) has been swapped with TransformerEngine implementation (`TELlamaDecoderLayer` or `TransformerLayer`), let's see how `FP8` training helps improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "458e55e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:836: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "# Minimize the bloat by wrapping all the imports in a function and return\n",
    "from utils import *\n",
    "\n",
    "\n",
    "## Default hyperparams, also defined in `utils.py` in class `Hyperparameters`\n",
    "# hyperparams.dataset_name = \"timdettmers/openassistant-guanaco\"\n",
    "# hyperparams.dataset_text_field = \"text\"\n",
    "# hyperparams.learning_rate = 1.41e-5\n",
    "# hyperparams.batch_size = 8\n",
    "# hyperparams.max_seq_length = 256\n",
    "# hyperparams.gradient_accumulation_steps = 1\n",
    "# hyperparams.num_training_steps=10\n",
    "\n",
    "\n",
    "## !!! `model_name` attr must point to the location of the model weights !!!\n",
    "hyperparams.model_name = \"/ckpt/llama-7bf-hf/\" # <== Add model weight location here\n",
    "hyperparams.mixed_precision = \"fp8\"\n",
    "\n",
    "\n",
    "## Init the model and accelerator wrapper\n",
    "model = init_te_llama_model(hyperparams)\n",
    "accelerator, model, optimizer, train_dataloader, lr_scheduler = wrap_with_accelerator(model, hyperparams)\n",
    "\n",
    "\n",
    "## Finetune the model\n",
    "finetune_model(model, hyperparams, accelerator, train_dataloader, optimizer, lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6948fc3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: loss: 2.2819559574127197, batch shape: torch.Size([8, 256]), peak gpu mem: 26.61 GB\n",
      "Step 0 time 3.093244791030884\n",
      "Step 1: loss: 3.3127150535583496, batch shape: torch.Size([8, 256]), peak gpu mem: 64.54 GB\n",
      "Step 1 time 0.7673726081848145\n",
      "Step 2: loss: 3.993682384490967, batch shape: torch.Size([8, 256]), peak gpu mem: 64.54 GB\n",
      "Step 2 time 0.23313021659851074\n",
      "Step 3: loss: 3.22248911857605, batch shape: torch.Size([8, 256]), peak gpu mem: 64.54 GB\n",
      "Step 3 time 0.23214364051818848\n",
      "Step 4: loss: 3.0897724628448486, batch shape: torch.Size([8, 256]), peak gpu mem: 64.54 GB\n",
      "Step 4 time 0.23146915435791016\n",
      "Step 5: loss: 3.6932754516601562, batch shape: torch.Size([8, 256]), peak gpu mem: 64.54 GB\n",
      "Step 5 time 0.23121118545532227\n",
      "Step 6: loss: 5.0839033126831055, batch shape: torch.Size([8, 256]), peak gpu mem: 64.54 GB\n",
      "Step 6 time 0.23106837272644043\n",
      "Step 7: loss: 3.907317638397217, batch shape: torch.Size([8, 256]), peak gpu mem: 64.54 GB\n",
      "Step 7 time 0.23175501823425293\n",
      "Step 8: loss: 3.2384896278381348, batch shape: torch.Size([8, 256]), peak gpu mem: 64.54 GB\n",
      "Step 8 time 0.23119401931762695\n",
      "Step 9: loss: 2.6323776245117188, batch shape: torch.Size([8, 256]), peak gpu mem: 64.54 GB\n",
      "Step 9 time 0.23113751411437988\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model.train()\n",
    "completed_steps = 0\n",
    "total_loss = 0\n",
    "optimizer.zero_grad()\n",
    "\n",
    "for _ in range(10):\n",
    "    if completed_steps >= num_training_steps:\n",
    "        break\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        start_time = time.time()\n",
    "        with accelerator.accumulate(model):\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            print(f\"Step {step}: loss: {loss.item()}, batch shape: {batch['input_ids'].shape}, peak gpu mem: {torch.cuda.max_memory_allocated() / 1024**3:.2f} GB\")\n",
    "            total_loss += loss.detach().float()\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if accelerator.sync_gradients:\n",
    "            completed_steps += 1\n",
    "\n",
    "        end_time = time.time()\n",
    "        total_time = end_time - start_time\n",
    "        print(f\"Step {step} time {total_time}\")\n",
    "        accelerator.log({\"batch_time\": total_time, \"input_ids\": batch[\"input_ids\"].cpu().numpy(), \"attention_mask\": batch[\"attention_mask\"].cpu().numpy()})\n",
    "        start_time = end_time\n",
    "\n",
    "        if completed_steps >= num_training_steps:\n",
    "            break\n",
    "\n",
    "accelerator.end_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f991f0c",
   "metadata": {},
   "source": [
    "| Models                                                      | Precision | Step Time (or ms per batch) | Speedup (over baseline) |\n",
    "|-------------------------------------------------------------|-----------|-----------------------------|-------------------------|\n",
    "| HF (baseline)                                               | BF16      | 288                         | 1                       |\n",
    "| HF (replace `nn.Linear` with `TE.Linear`)                   | FP8       | 307                         | 0.94                    |\n",
    "| TE (replace `LlamaDecoderLayer` with `TE.TransformerLayer`) | BF16      | 243                         | 1.19                    |\n",
    "| TE (replace `LlamaDecoderLayer` with `TE.TransformerLayer`) | FP8       | 231                         | 1.24                    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840d2da3",
   "metadata": {},
   "source": [
    "## Benchmarks (revisited)\n",
    "Let's take a look at the summary of the performance numbers with various configurations.\n",
    "\n",
    "| Models                                                      | Precision | Step Time (or ms per batch) | Speedup (over baseline) |\n",
    "|-------------------------------------------------------------|-----------|-----------------------------|-------------------------|\n",
    "| HF (baseline)                                               | BF16      | 288                         | 1                       |\n",
    "| HF (replace `nn.Linear` with `TE.Linear`)                   | FP8       | 307                         | 0.94                    |\n",
    "| TE (replace `LlamaDecoderLayer` with `TE.TransformerLayer`) | BF16      | 243                         | 1.19                    |\n",
    "| TE (replace `LlamaDecoderLayer` with `TE.TransformerLayer`) | FP8       | 231                         | 1.24                    |\n",
    "\n",
    "When we use larger chunks of layers from TransformerEngine, we see larger performance improvement. Further, when `FP8` precision is enabled, we see even larger speedup as is expected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57611fbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
